#+PROPERTY:  session *R*
#+PROPERTY:  tangle yes
#+OPTIONS: ^:nil


#+begin_src R :exports none
  ##################################################################
  ## Source code for the book: "Displaying time series, spatial and
  ## space-time data with R: stories of space and time"
  
  ## Copyright (C) 2013-2012 Oscar Perpiñán Lamigueiro
  
  ## This program is free software you can redistribute it and/or modify
  ## it under the terms of the GNU General Public License as published
  ## by the Free Software Foundation; either version 2 of the License,
  ## or (at your option) any later version.
   
  ## This program is distributed in the hope that it will be useful, but
  ## WITHOUT ANY WARRANTY; without even the implied warranty of
  ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  ## General Public License for more details.
   
  ## You should have received a copy of the GNU General Public License
  ## along with this program; if not, write to the Free Software
  ## Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA
  ## 02111-1307, USA.
  ####################################################################
#+end_src

#+begin_src R :exports none
  ## Set folder to where the local copy of github repository can be found
  setwd('~/Dropbox/chapman/book/')
  
  library(lattice)
  library(ggplot2)
  library(latticeExtra)
  
  myTheme <- custom.theme.2(pch=19, cex=0.7,
                            region=rev(brewer.pal(9, 'YlOrRd')),
                            symbol = brewer.pal(n=8, name = "Dark2"))
  myTheme$strip.background$col='transparent'
  myTheme$strip.shingle$col='transparent'
  myTheme$strip.border$col='transparent'
  
  xscale.components.custom <- function(...){
      ans <- xscale.components.default(...)
      ans$top=FALSE
      ans}
  yscale.components.custom <- function(...){
      ans <- yscale.components.default(...)
      ans$right=FALSE
      ans}
  myArgs <- list(as.table=TRUE,
                 between=list(x=0.5, y=0.2),
                 xscale.components = xscale.components.custom,
                 yscale.components = yscale.components.custom)
  defaultArgs <- lattice.options()$default.args
  
  lattice.options(default.theme = myTheme,
                  default.args = modifyList(defaultArgs, myArgs))
  
#+end_src

* Thematic Maps
\label{sec:thematicMaps}
#+begin_src R :exports none
  ##################################################################
  ## Thematic maps
  ##################################################################
#+end_src

A thematic map focuses on a specific theme or variable commonly
using geographic data such as coastlines, boundaries and places,
as points of reference for the variable being mapped. These maps
provide specific information about particular locations or areas
(proportional symbol mapping and choropleth maps) and information
about spatial patterns (isarithmic and raster maps).

** Choropleth maps
\label{sec:multiChoropleth}
#+begin_src R :exports none
  ##################################################################
  ## Choropleth maps
  ##################################################################
#+end_src

A choropleth map shades regions according to the measurement of a
variable displayed on the map. The choropleth map is an appropiate
tool to visualize a variable uniformly distributed within each
region, changing only at the region boundaries. This methods
performs correctly with homogeneous regions both in size and
shape.  

This section details how to create a multivariate choropleth map
to show the results of the 2011 Spanish general elections. It is
inspired by the infographic from the New York Times[fn:3], a
multivariate choropleth map of the inmigration behaviour in the
USA.

#+begin_src R 
  votes2011 <- read.csv('data/votes2011.csv',
                        colClasses=c('factor', 'factor', 'numeric', 'numeric'))
#+end_src

Next section describes how to define a =SpatialPolygonsDataFrame=
with the data from this =data.frame= and the spatial information
of the administrative boundaries from a shapefile. You can skip it
for a later reading if you are not interested in this procedure
and jump to the section \ref{sec:map} where the maps are produced.

*** \floweroneleft Administrative boundaries

#+begin_src R :exports none
  ##################################################################
  ## Administrative boundaries
  ##################################################################
#+end_src

The Spanish administrative boundaries are available as shapefiles at
the INE webpage[fn:7]. Both the municipalities, =espMap=, and provinces
boundaries, =provinces=, are read as =SpatialPolygonsDataFrame= with
=readShapePoly=.

\index{Packages!maps@\texttt{maps}}
\index{Packages!maptools@\texttt{maptools}}
\index{Packages!rgeos@\texttt{rgeos}}
\index{Packages!sp@\texttt{sp}}
\index{Packages!latticeExtra@\texttt{latticeExtra}}
\index{Packages!colorspace@\texttt{colorspace}}
#+begin_src R
  library(sp)
  library(maptools)
#+end_src

\index{INE}
\index{readShapePoly@\texttt{readShapePoly}}
\index{Encoding@\texttt{Encoding}}
#+begin_src R :eval no-export
  old <- setwd(tempdir())
  download.file('http://goo.gl/TIvr4', 'mapas_completo_municipal.rar')
  system2('unrar', c('e', 'mapas_completo_municipal.rar'))
  espMap <- readShapePoly(fn="esp_muni_0109")
  Encoding(levels(espMap$NOMBRE)) <- "latin1"
  
  provinces <- readShapePoly(fn="spain_provinces_ag_2")
  setwd(old)
#+end_src

#+begin_src R :exports none :tangle no
  espMap <- readShapePoly(fn="~/Datos/mapas_completo_municipal/esp_muni_0109")
  Encoding(levels(espMap$NOMBRE)) <- "latin1"
  
  provinces <- readShapePoly(fn="~/Datos/mapas_completo_municipal/spain_provinces_ag_2")
#+end_src  

Some of the polygons are repeated and can be dissolved with =unionSpatialPolygons=. 
\index{unionSpatialPolygons@\texttt{unionSpatialPolygons}}
#+begin_src R 
  ## dissolve repeated polygons
  espPols <- unionSpatialPolygons(espMap, espMap$PROVMUN) 
#+end_src

Spanish maps are commonly displayed with the Canarian islands next
to the peninsula. First we have to extract the polygons of the
islands and the polygons of the peninsula, and then shift the
coordinates of the islands with =elide=. Finally, a new
=SpatialPolygons= object binds the shifted islands with the
peninsula.

#+begin_src R
  ## Extract Canarias islands from the SpatialPolygons object
  canarias <-  sapply(espPols@polygons, function(x)substr(x@ID, 1, 2) %in% c("35",  "38"))
  peninsulaPols <- espPols[!canarias]
  islandPols <- espPols[canarias]
  
  ## Shift the island extent box to position them at the bottom right corner
  dy <- bbox(peninsulaPols)[2,1] - bbox(islandPols)[2,1]
  dx <- bbox(peninsulaPols)[1,2] - bbox(islandPols)[1,2]
  islandPols2 <- elide(islandPols, shift=c(dx, dy))
  bbIslands <- bbox(islandPols2)
  
  ## Bind Peninsula (without islands) with shifted islands
  espPols <- rbind(peninsulaPols, islandPols2)
#+end_src

Last step is to link the data with the polygons. The =ID= slot of
each polygon is the key to find the correspondent registry in the
=votes2011= dataset.
#+begin_src R
  ## Match polygons and data using ID slot and PROVMUN column
  IDs <- sapply(espPols@polygons, function(x)x@ID)
  idx <- match(IDs, votes2011$PROVMUN)
  
  ##Places without information
  idxNA <- which(is.na(idx))
  
  ##Information to be added to the SpatialPolygons object
  dat2add <- votes2011[idx, ]
  
  ## SpatialPolygonsDataFrame uses row names to match polygons with data
  row.names(dat2add) <- IDs
  espMapVotes <- SpatialPolygonsDataFrame(espPols, dat2add)
  
  ## Drop those places without information
  espMapVotes <- espMapVotes[-idxNA, ]
#+end_src

*** Map
\label{sec:map}
#+begin_src R :exports none
  ##################################################################
  ## Map
  ##################################################################
#+end_src

The =SpatialPolygonsDataFrame= constructed in the previous section
contains two main variables: =whichMax=, the name of the
predominant political option, and =pcMax=, percentage of votes
obtained by this political option.

=whichMax= is a categorical value with four levels: the two main
parties (=PP= and =PSOE=), the abstention results (=ABS=), and the
rest of parties (=OTH=). The figure \ref{fig:whichMax} encodes
these levels with a qualitative palette with constant hues and
varying chroma and luminance for each class. In order to improve
the color discrimination, hues are equally spaced along the
HCL-based color wheel.

\index{rainbow_hcl@\texttt{rainbow\_hcl}}
#+begin_src R 
  library(colorspace)  
  
  classes <- levels(factor(espMapVotes$whichMax))
  nClasses <- length(classes)
  
  qualPal <- rainbow_hcl(nClasses, start=30, end=300)
#+end_src

For the definition of a combined palette in the next section, it
is interesting to note that the colours provided by =rainbow_hcl=
can be obtained with the next code where the distances between
hues and their values are computed explicitely.
\index{hcl@\texttt{hcl}}
#+begin_src R 
  ## distance between hues
  step <- 360/nClasses 
  ## hues equally spaced
  hue = (30 + step*(seq_len(nClasses)-1))%%360 
  qualPal <- hcl(hue, c=50, l=70)
#+end_src

#+CAPTION: Categorical choropleth map displaying the name of the predominant political option in each municipality in the 2011 Spanish General Elections.
#+LABEL: fig:whichMax
#+begin_src R :results output graphics :exports both :file figs/whichMax.pdf
  spplot(espMapVotes["whichMax"], col='transparent', col.regions=qualPal)
#+end_src

On the other hand, =pcMax= is a quantitative variable which can be
adequately displayed with a sequential palette (figure \ref{fig:pcMax}).
#+CAPTION: Quantitative choropleth map displaying the percentage of votes obtained by the predominant political option in each municipality in the 2011 Spanish General Elections.
#+LABEL: fig:pcMax
#+begin_src R :results output graphics :exports both :file figs/pcMax.pdf
  quantPal <- rev(heat_hcl(16))
  spplot(espMapVotes["pcMax"], col='transparent', col.regions=quantPal)
#+end_src

*** \floweroneleft Categorical and quantitative variables combined in a multivariate choropleth map

Following the inspiring example of the infographic from the New
York Times, we will combine both choropleth maps to produce a
multivariate map: the hue of each polygon will be determined by
the name of the predominant option (=whichMax=) but the chroma and
luminance will vary according to the percentage of votes
(=pcMax=). Hues are computed with the same method as in the figure
\ref{fig:whichMax}, while the correspondent values of chroma and
luminance are calculated with the =sequential_hcl= function.

\index{sequential_hcl@\texttt{sequential\_hcl}}
#+begin_src R
  classes <- levels(factor(espMapVotes$whichMax))
  nClasses <- length(classes)
  step <- 360/nClasses
  multiPal <- lapply(1:nClasses, function(i){
      rev(sequential_hcl(16, h = (30 + step*(i-1))%%360))
      })
#+end_src

With this multivariate palette we can produce a list of maps
extracting the polygons according to each class and filling with
the appropiate colour from this palette. The resulting list of
=trellis= objects can be combined with =Reduce= and the
=+.trellis= function of the =latticeExtra= and produce a =trellis=
object.

It is important to note that, to ensure the legends homogeneity,
the breakpoints defined by the =at= argument are the same for all
the individual maps.

\index{Reduce@\texttt{Reduce}} \index{spplot@\texttt{spplot}}
#+begin_src R 
  pList <- lapply(1:nClasses, function(i){
      ## Only those polygons corresponding to a level are selected
      mapClass <- espMapVotes[espMapVotes$whichMax==classes[i],]
      pClass <- spplot(mapClass['pcMax'], col.regions=multiPal[[i]],
                       col='transparent',
                       at = seq(0, 100, by=20))
  })
  
  p <- Reduce('+', pList)
#+end_src

The legend of this =trellis= object has to be defined
manually. The main operation is to merge the legends from the
components of the list of maps to obtain a bivariate
legend. 

The first step is to add a title to each individual legend.  This
is a little complex because =levelplot= (the engine under the
=spplot= method) does not include a title in their color key. The
solution is to define a function to add the title and include it
as an argument to the legend component of each =trellis=
objet. The =print.trellis= method will process this function when
displaying the =trellis= object. The =frameGrob= and =packGrob= of
the =grid= package will do the main work inside this function.

\index{textGrob@\texttt{textGrob}}
\index{packGrob\texttt{packGrob}}
\index{Packages!grid\texttt{grid}}
#+begin_src R
  ## Function to add a title to a legend
  addTitle <- function(legend, title){
    titleGrob <- textGrob(title, gp=gpar(fontsize=8), hjust=1, vjust=1)
    ## retrieve the legend from the trellis object
    legendGrob <- eval(as.call(c(as.symbol(legend$fun), legend$args)))
    ## Layout of the legend WITH the title
    ly <- grid.layout(ncol=1, nrow=2,
                      widths=unit(0.9, 'grobwidth', data=legendGrob))
    ## Create a frame to host the original legend and the title
    fg <- frameGrob(ly, name=paste('legendTitle', title, sep='_'))
    ## Add the grobs to the frame
    pg <- packGrob(fg, titleGrob, row=2)
    pg <- packGrob(pg, legendGrob, row=1)
    }
  
  ## Access each trellis object from pList...
  for (i in seq_along(classes)){
    ## extract the legend (automatically created by spplot)...
    lg <- pList[[i]]$legend$right
    ## ...supress labels except from the last legend...
    lg$args$key$labels$cex=ifelse(i==nClasses, 0.8, 0) 
    ## ... and add the addTitle function to the legend component of each trellis object
    pList[[i]]$legend$right <- list(fun='addTitle',
                                    args=list(legend=lg, title=classes[i]))
  }
#+end_src

Now that every component of =pList= includes a legend with a title
the legend of the =p= trellis object can be modified to store the
merged legends from the set of components of =pList=.

#+begin_src R
  ## List of legends
  legendList <- lapply(pList, function(x){
    lg <- x$legend$right
    clKey <- eval(as.call(c(as.symbol(lg$fun), lg$args)))
    clKey
  })
  
  ## Function to pack the list of legends in a unique legend
  ## Adapted from latticeExtra::: mergedTrellisLegendGrob
  packLegend <- function(legendList){
    N <- length(legendList)
    ly <- grid.layout(nrow = 1,  ncol = N)
    g <- frameGrob(layout = ly, name = "mergedLegend")
    for (i in 1:N) g <- packGrob(g, legendList[[i]], col = i)
    g
  }
  
  ## The legend of p will include all the legends
  p$legend$right <- list(fun = 'packLegend',  args = list(legendList = legendList))
#+end_src

The figure \ref{fig:mapLegends} displays the result with the provinces
boundaries superposed (only for the peninsula due to a problem with
the definition of boundaries the Canarian islands in the file) and a
rectangle to separate the Canarian islands from the rest of the map.

#+CAPTION: Spanish General Elections results. The map shows the result of the most voted option in each municipality.
#+LABEL: fig:mapLegends
#+begin_src R :results output graphics :exports both :file figs/mapLegends.pdf
  canarias <- provinces$PROV %in% c(35, 38)
  peninsulaLines <- provinces[!canarias,]
  
  p +
    layer(sp.polygons(peninsulaLines,  lwd = 0.1)) +
    layer(grid.rect(x=bbIslands[1,1], y=bbIslands[2,1],
                    width=diff(bbIslands[1,]),
                    height=diff(bbIslands[2,]),
                    default.units='native', just=c('left', 'bottom'),
                    gp=gpar(lwd=0.5, fill='transparent')))
#+end_src

#+RESULTS:
[[file:figs/mapLegends.pdf]]

** Raster maps
#+begin_src R :exports none
  ##################################################################
  ## Raster maps
  ##################################################################
#+end_src

A raster data structure is a matrix of cells organized into rows
and columns where each cell contains a value representing
information, such as temperature, altitude, population density,
landuse, etc.  This section describes how to display a raster with
two different examples: CM-SAF irradiation rasters will illustrate
the use of diverging palettes; land cover and population data from
the NEO-NASA project will exemplify the display of categorical
data and multivariate rasters.

*** Diverging palettes 
#+begin_src R :exports none
  ##################################################################
  ## Diverging palettes
  ##################################################################
#+end_src

The =RasterLayer= object of annual averages of irradiation
estimated by CM-SAF can be easily displayed with the =levelplot=
method of the =rasterVis= package.

\index{Packages!raster@\texttt{raster}}
\index{Packages!rasterVis@\texttt{rasterVis}}
\index{levelplot@\texttt{levelplot}}
\index{rasterTheme@\texttt{rasterTheme}}
#+CAPTION: Annual average of solar radiation displayed with a sequential palette.
#+LABEL: fig:levelplotCMSAF
#+begin_src R :results output graphics :exports both :file figs/leveplotSISavOrig.pdf
  library(raster)
  library(rasterVis)
  SISav <- raster('data/SISav')
  levelplot(SISav)
#+end_src

However, instead of displaying the absolute values of each cell we
will analyze the differences between each cell and the global
average value. This average is computed with the =cellStats=
function and substracted from the original =RasterLayer=. The
figure \ref{fig:xyplotSISav} displays the relation between these
scaled values and latitude (=y=), with 5 different groups defined
by the longitude (=cut(x, 5)=). It is evident that larger
irradiation values are associated with lower latitudes. However,
there is not such a clear relation between irradiation and
longitude.

\index{cellStats@\texttt{cellStats}}
#+begin_src R
meanRad <- cellStats(SISav, 'mean')
SISav <- SISav - meanRad
#+end_src

\index{xyplot@\texttt{xyplot}}
\index{rasterTheme@\texttt{rasterTheme}}
\index{Packages!hexbin@\texttt{hexbin}}
\index{plinrain@\texttt{plinrain}}
#+CAPTION: Relation between scaled annual average radiation and latitude for several longitude groups.
#+LABEL: fig:xyplotSISav
#+begin_src R :results output graphics :exports both :width 2000 :height 2000 :res 300 :file figs/xyplotSISav.png 
  xyplot(layer ~ y, data = SISav,
         groups=cut(x, 5),
         par.settings=rasterTheme(symbol=plinrain(n=5, end=200)),
         xlab = 'Latitude', ylab = 'Solar radiation (scaled)',  
         auto.key=list(space='right', title='Longitude', cex.title=1.3))
#+end_src

Numerical information ranging in an interval including a neutral
value is commonly displayed with diverging palettes. These
palettes represent neutral classes with light colors, while low
and high extremes of the data range are highlighted using dark
colors with contrasting hues. I use the Purple-Orange palette from
ColorBrewer with purple for positive values and orange for
negative values. In order to underline the position of the
interval containing zero, the center colour of this palette is
substituted with pure white. The resulting palette is displayed in
the figure \ref{fig:showDivPal} with the custom =showPal=
function. The correspondent raster map produced with this palette
is displayed in the figure \ref{fig:divPal_SISav_naive}.  Although
extreme positive and negative values can be easily discriminated,
the zero value is not associated with white because the data range
is not symmetrical around zero.

\index{Package!RColorBrewer@\texttt{RColorBrewer}}
\index{brewer.pal@\texttt{brewer.pal}}
#+CAPTION: Purple-Orange diverging palette using white as middle color.
#+LABEL: fig:showDivPal
#+ATTR_LaTeX: width=0.35\textwidth
#+begin_src R :results output graphics :exports both :file figs/showDivPal.pdf
  divPal <- brewer.pal(n=9, 'PuOr')
  divPal[5] <- "#FFFFFF"
  
  showPal <- function(pal, labs=pal, cex=0.6, ...){
    barplot(rep(1, length(pal)), col=pal,
            names.arg=labs, cex.names=cex,
            axes=FALSE, ...)
  }
  
  showPal(divPal)
#+end_src


#+CAPTION: Asymmetric raster data (scaled annual average irradiation) displayed with a symmetric diverging palette.
#+LABEL: fig:divPal_SISav_naive
#+begin_src R :results output graphics :exports both :file figs/divPal_SISav_naive.pdf
  divTheme <- rasterTheme(region=divPal)
  
  levelplot(SISav, contour=TRUE, par.settings=divTheme)
#+end_src

The solution is to connect the symmetrical colour palette with the
asymmetrical data range. The first step is to create a set of
breaks such that the zero value is the center of one of the
intervals.
#+begin_src R 
  rng <- range(SISav[])
  ## Number of desired intervals
  nInt <- 15
  ## Increment correspondent to the range and nInt
  inc0 <- diff(rng)/nInt
  ## Number of intervals from the negative extreme to zero
  n0 <- floor(abs(rng[1])/inc0)
  ## Update the increment adding 1/2 to position zero in the center of an interval
  inc <- abs(rng[1])/(n0 + 1/2)
  ## Number of intervals from zero to the positive extreme
  n1 <- ceiling((rng[2]/inc - 1/2) + 1)
  ## Collection of breaks
  breaks <- seq(rng[1], by=inc, length= n0 + 1 + n1)
#+end_src

Next step is to compute the midpoints of each interval. These
points represent the data belonging to each interval and their
value will be connected with a color of the palette.
\index{findInterval@\texttt{findInterval}}
\index{tapply@\texttt{tapply}}
#+begin_src R 
  ## Midpoints computed with the median of each interval
  idx <- findInterval(SISav[], breaks, rightmost.closed=TRUE)
  mids <- tapply(SISav[], idx, median)
  ## Maximum of the absolute value both limits
  mx <- max(abs(breaks))
  mids
#+end_src

A simple method to relate the palette and the intervals is with a
straight line such that a point is defined by the absolute maximum
value, (=(mx, 1)=), and another point by zero, (=(0, 0.5)=).  Why
are we using the interval [0, 1] as the =y= coordinate of this
line, and why is 0.5 the result of zero? The reason is that the
input of the =break2pal= function will be the result of
=colorRamp=, a function that creates another interpolating
function that maps colors with values between 0 and 1. Therefore,
a new palette is created extracting colors from the original
palette, such that the central color (white) is associated with
the interval containing zero. This palette is displayed in the
figure \ref{fig:showBreak2Pal}.

The raster map produced with this new palette is displayed in the
figure \ref{fig:divPalSISav}. Now zero is clearly associated with the
white color.
\index{colorRamp\texttt{colorRamp}}
\index{rgb@\texttt{rgb}}
#+CAPTION: Modified diverging palette related with the asymmetrical raster data.
#+LABEL: fig:showBreak2Pal
#+ATTR_LaTeX: width=0.5\textwidth
#+begin_src R :results output graphics :exports both :file figs/showBreak2Pal.pdf
  break2pal <- function(x, mx, pal){
    ## x = mx gives y = 1
    ## x = 0 gives y = 0.5
    y <- 1/2*(x/mx + 1)
    rgb(pal(y), maxColorValue=255)
  }
  
  ## Interpolating function that maps colors with [0, 1]
  ## rgb(divRamp(0.5), maxColorValue=255) gives "#FFFFFF" (white)
  divRamp <- colorRamp(divPal)
  ## Diverging palette where white is associated with the interval
  ## containing the zero
  pal <- break2pal(mids, mx, divRamp)
  showPal(pal, round(mids, 1))
#+end_src


#+CAPTION: Asymmetric raster data (scaled annual average irradiation) displayed with a modified diverging palette.
#+LABEL: fig:divPalSISav
#+begin_src R :results output graphics :exports both :file figs/divPalSISav.pdf
  levelplot(SISav, par.settings=rasterTheme(region=pal),
            at=breaks, contour=TRUE)
#+end_src


It is interesting to note two operations carried out internally by
the =lattice= package. First, the =custom.theme= function (used by
=rasterTheme=) creates a new palette with 100 colours using
=colorRampPalette= to interpolate the palette passed as an
argument. Second, the =level.colors= function makes the
arrangement between intervals and colors. If this function
receives more colors than intervals, it chooses a subset of the
palette disregarding some of the intermediate colors. Therefore,
since this function will receive 100 colors from =par.settings= it
is difficult to control exactly which colors of our original
palette will be represented.

An alternative way for finer control is to fill with our palette
the =regions$col= component of the theme after it has been
created.

#+CAPTION: Same as figure \ref{fig:divPalSISav} but colors are assigned directly to the =regions$col= component of the theme.
#+LABEL: fig:divPal_SISav_regions
#+begin_src R :results output graphics :exports both :file figs/divPalSISav_regions.pdf
  divTheme <- rasterTheme()
  
  divTheme$regions$col <- pal
  levelplot(SISav, par.settings=divTheme, at=breaks, contour=TRUE)
#+end_src

A final improvement to this map is to compute the intervals using
a classification algorithm with the =classInt= package. With this
approach it is likely that zero will not be perfectly centered in
its correspondent interval. The rest of the code is exactly the
same as above replacing the =breaks= vector with the result of the
=classIntervals= function. The figure
\ref{fig:divPalSISav_classInt} displays the result.

\index{Packages!classInt@\texttt{classInt}}
\index{classIntervals@\texttt{classIntervals}}
#+begin_src R 
  library(classInt)
  
  cl <- classIntervals(SISav[],
                       ## n=15, style='equal')
                       ## style='hclust')
                       ## style='sd')
                       style='kmeans')
                       ## style='quantile')
  cl
  breaks <- cl$brks
#+end_src

#+CAPTION: Same as figure \ref{fig:divPal_SISav_regions} but defining intervals with the optimal classification method.
#+LABEL: fig:divPalSISav_classInt
#+begin_src R :results output graphics :exports both :file figs/divPalSISav_classInt.pdf
  idx <- findInterval(SISav[], breaks, rightmost.closed=TRUE)
  mids <- tapply(SISav[], idx, median)
  mids
  mx <- max(abs(breaks))
  pal <- break2pal(mids, mx, divRamp)
  divTheme$regions$col <- pal
  levelplot(SISav, par.settings=divTheme, at=breaks, contour=TRUE)
#+end_src


*** Categorical data
#+begin_src R :exports none
  ##################################################################
  ## Categorical data
  ##################################################################
#+end_src

Land cover is the observed physical cover on the earth's
surface. A set of 17 different categories is commonly used. Using
satellite observations, it is possible to map where on Earth each of
these 17 land surface categories can be found and how these land
covers change over time. This raster is a typical example of
categorical data. 

This section illustrates how to read and display rasters with
categorical information using information from the NEO-NASA
project. After the land cover and population density files have
been downloaded, two =RasterLayers= can be created with the
=raster= package. Both files are read, their geographical extent
reduced to the area of India and China, and cleaned (99999 cells
are replaced with =NA=).

\index{Packages!raster@\texttt{raster}}
\index{extent@\texttt{extent}}
\index{crop@\texttt{crop}}
#+begin_src R :eval no-export
  library(raster)
  ## China and India  
  ext <- extent(65, 135, 5, 55)
  
  pop <- raster('875430rgb-167772161.0.FLOAT.TIFF')
  pop <- crop(pop, ext)
  pop[pop==99999] <- NA
  
  landClass <- raster('241243rgb-167772161.0.TIFF')
  landClass <- crop(landClass, ext)
#+end_src

#+begin_src R :exports none :tangle no
  library(raster)
  
  
  ext <- extent(65, 135, 5, 55)
  
  pop <- raster('~/Datos/Nasa/875430rgb-167772161.0.FLOAT.TIFF')
  pop <- crop(pop, ext)
  pop[pop==99999] <- NA
  
  landClass <- raster('~/Datos/Nasa/241243rgb-167772161.0.TIFF')
  landClass <- crop(landClass, ext)
#+end_src

The codes of the classification are graphically described in the
figure \ref{fig:lccKey}. In summary, the sea is labeled with 0,
forests with 1 to 5, shrublands, grasslands and wetlands with 6 to
11, agriculture and urban lands with 12 to 14, and snow and barren
with 15 and 16.  These four groups (sea is replaced =NA=) will be
the levels of the categorical raster. The =raster= package
includes the =ratify= method to define a layer as categorical data
filling it with integer values associated to a Raster Attribute
Table (RAT).

#+BEGIN_LaTeX
\begin{figure}
\includegraphics[width=0.3\textwidth]{figs/lcc_key.jpg}
\caption{\label{fig:lccKey}Codes of land cover classification}
\end{figure}
#+END_LaTeX

\index{ratify@\texttt{ratify}}
\index{cut@\texttt{cut}}
#+begin_src R
  landClass[landClass %in% c(0, 254)] <- NA
  ## Only four groups are needed:
  ## Forests: 1:5
  ## Shublands, etc: 6:11
  ## Agricultural/Urban: 12:14
  ## Snow: 15:16
  landClass <- cut(landClass, c(0, 5, 11, 14, 16))
  ## Add a Raster Atribute Table and define the raster as categorical data
  landClass <- ratify(landClass)
  ## Configure the RAT: first create a RAT data.frame using the
  ## levels method; second, set the values for each class (to be
  ## used by levelplot); third, assign this RAT to the raster
  ## using again levels
  rat <- levels(landClass)[[1]]
  rat$classes <- c('Forest', 'Land', 'Urban', 'Snow')
  levels(landClass) <- rat
#+end_src

This categorical raster can be easily displayed with the
=levelplot= method of the =rasterVis= package. Previously, a theme
is defined with the background color set to =lightskyblue1= to
display the sea areas (filled with =NA= values), and the region
palette is defined with adequate colors.

\index{Packages!rasterVis@\texttt{rasterVis}}
\index{levelplot@\texttt{levelplot}}
\index{modifyList@\texttt{modifyList}}
\index{rasterTheme@\texttt{rasterTheme}}
#+CAPTION: Land cover raster (categorical data).
#+LABEL: fig:landClass
#+begin_src R :results output graphics :exports both :file figs/landClass.pdf
  library(rasterVis)
  
  pal <- c('palegreen4', # Forest
           'lightgoldenrod', # Land
           'indianred4', # Urban
           'snow3')      # Snow
  
  catTheme <- modifyList(rasterTheme(),
                         list(panel.background = list(col='lightskyblue1'),
                              regions = list(col= pal)))
  
  levelplot(landClass, maxpixels=3.5e5, par.settings=catTheme,
            panel=panel.levelplot.raster)
#+end_src

Let's explore the relation between the land cover and population
density rasters. Figure \ref{fig:populationNASA} displays this
last raster using a logarithmic scale.

#+CAPTION: Population density raster.
#+LABEL: fig:populationNASA
#+begin_src R :results output graphics :exports both :file figs/populationNASA.pdf
  pPop <- levelplot(pop, zscaleLog=10, par.settings=BTCTheme,
                    maxpixels=3.5e5, panel=panel.levelplot.raster)
  pPop
#+end_src

Both rasters can be joined together with the =stack= method to
create a new =RasterStack= object. Figure
\ref{fig:histogramLandClass} displays the distribution of the
logarithm of the population density associated to each land class.

\index{stack@\texttt{stack}}
\index{histogram@\texttt{histogram}}
#+CAPTION: Distribution of the logarithm of the population density associated to each land class.
#+LABEL: fig:histogramLandClass
#+begin_src R :results output graphics :exports both :file figs/histogramLandClass.pdf
  s <- stack(pop, landClass)
  names(s) <- c('pop', 'landClass')
  histogram(~log10(pop)|landClass, data=s,
            scales=list(relation='free'))
#+end_src


*** \floweroneleft  Multivariate legend
We can reproduce the code used to create the multivariate
choropleth (section \ref{sec:multiChoropleth}) using the
=levelplot= function from the =rasterVis= package. Again, the
result is a list of =trellis= objects. Each of these objects is
the representation of the population density in a particular land
class. The =+.trellis= function of the =latticeExtra= package with
=Reduce= superposes the elements of this list and produce a
=trellis= object. Figure \ref{fig:popLandClass} displays the
result.

# #+begin_src R 
#   library(colorspace)
  
#   col2hcl <- function(col){
#     rgb <- t(col2rgb(col))/256
#     luv <- convertColor(rgb, 'sRGB', 'Luv')
#     coords <- as(LUV(luv), 'polarLUV')@coords
#     coords
#     }

#  cols <- colorRampPalette(c('white', pal[i]),  space='Lab')(100)
#  hclPal <- col2hcl(pal[i])
#  cols <- rev(sequential_hcl(100, h=hclPal[1], c=c(hclPal[2], 0), l=c(hclPal[3], 90)))
# #+end_src

#+begin_src R 
  ## at for each sub-levelplot is obtained from the global levelplot
  at <- pPop$legend$bottom$args$key$at
  classes <- rat$classes
  nClasses <- length(classes)
  
  pList <- lapply(1:nClasses, function(i){
    landSub <- landClass
    ## Those cells from a different land class are set to NA...
    landSub[!(landClass==i)] <- NA
    ## ... and the resulting raster mask the population raster
    popSub <- mask(pop, landSub)
    ## The HCL color wheel is divided in nClasses
    step <- 360/nClasses
    ## and a sequential palette is constructed with a hue from one
    ## the color wheel parts
    cols <- rev(sequential_hcl(16, h = (30 + step*(i-1))%%360))
  
    pClass <- levelplot(popSub, zscaleLog=10, at=at, maxpixels=3.5e5,
                        col.regions=cols, margin=FALSE)
  })
  
#+end_src

#+CAPTION: Population density for each land class (multivariate raster).
#+LABEL: fig:popLandClass
#+begin_src R :results output graphics :exports results :width 2000 :height 2000 :res 300 :file figs/popLandClass.png
  p <- Reduce('+', pList)
  ## Function to add a title to a legend
  addTitle <- function(legend, title){
    titleGrob <- textGrob(title, gp=gpar(fontsize=8), hjust=1, vjust=1)
    ## retrieve the legend from the trellis object
    legendGrob <- eval(as.call(c(as.symbol(legend$fun), legend$args)))
    ## Layout of the legend WITH the title
    ly <- grid.layout(ncol=1, nrow=2,
                      widths=unit(0.9, 'grobwidth', data=legendGrob))
    ## Create a frame to host the original legend and the title
    fg <- frameGrob(ly, name=paste('legendTitle', title, sep='_'))
    ## Add the grobs to the frame
    pg <- packGrob(fg, titleGrob, row=2)
    pg <- packGrob(pg, legendGrob, row=1)
    }
  
  ## Access each trellis object from pList...
  for (i in seq_len(nClasses)){
    ## extract the legend (automatically created by spplot)...
    lg <- pList[[i]]$legend$right
    ## ...supress labels except from the last legend...
    lg$args$key$labels$cex=ifelse(i==nClasses, 0.8, 0) 
    ## ... and add the addTitle function to the legend component of each trellis object
    pList[[i]]$legend$right <- list(fun='addTitle',
                                    args=list(legend=lg, title=classes[i]))
  }
  
  ## List of legends
  legendList <- lapply(pList, function(x){
    lg <- x$legend$right
    clKey <- eval(as.call(c(as.symbol(lg$fun), lg$args)))
    clKey
  })
  
  ## Function to pack the list of legends in a unique legend
  ## Adapted from latticeExtra::: mergedTrellisLegendGrob
  packLegend <- function(legendList){
    N <- length(legendList)
    ly <- grid.layout(nrow = 1,  ncol = N)
    g <- frameGrob(layout = ly, name = "mergedLegend")
    for (i in 1:N) g <- packGrob(g, legendList[[i]], col = i)
    g
  }
  
  ## The legend of p will include all the legends
  p$legend$right <- list(fun = 'packLegend',  args = list(legendList = legendList))
  
  p
#+end_src

#+RESULTS:
[[file:figs/popLandClass.png]]


** Proportional symbol mapping
#+begin_src R :exports none
  ##################################################################
  ## Proportional symbol mapping
  ##################################################################
#+end_src
   
*** Introduction
The proportional symbol technique uses symbols of different sizes
to represent data associated with areas or point locations, with
circles being the most frequently used geometric symbol. The data
and the size of symbols can be related through different types of
scaling: mathematical scaling sizes areas of point symbols in
direct proportion to the data; perceptual scaling corrects the
mathematical scaling to account for visual understimation of
larger symbols; and range grading, where data is grouped, and each
class is represented with a single symbol size. 

In this section we display the air quality data with circles as
proportional symbol[fn:4], and range grading as scaling method. The
objective when using range grading is to discriminate between
classes instead of estimating an exact value from a perceived
symbol size. However, because human perception of symbol size is
limited, it is always recommendable to add a second perception
channel to improve the discrimination task. Colours from a
sequential palette will complement symbol size to encode the
groups[fn:2].


## TODO: explicar paquetes que usamos.

*** Combine data and spatial locations

Our starting point is to retrieve and combine the data and spatial
information. The locations are contained in =airStations=, a
=data.frame= which is converted to an =SpatialPointsDataFrame=
object with the =coordinates= method. The =airQuality=
=data.frame= comprises the air quality daily measurements acquired
at each station during 2011. 

On the other hand, we define a wrapper function, =summarize=,
which passes several statistical functions to =aggregate= and
format the result adequately.  The results are included in the
=SpatialPointsDataFrame= with the =spCbind= method. In the next
section we will only display the $NO_2$ average values. The other
statistics will be included in an SVG version of the graphic with
tooltips to show additional information.

\index{Data!Air quality in Madrid}
\index{Packages!sp@\texttt{sp}}
\index{Packages!maptools@\texttt{maptools}}
\index{read.csv2@\texttt{read.csv2}}
\index{aggregate@\texttt{aggregate}} \index{match@\texttt{match}}
\index{spCbind@\texttt{spCbind}}
#+begin_src R 
  library(sp)
  library(maptools)
  
  ## Spatial location of stations
  airStations <- read.csv2('data/airStations.csv')
  coordinates(airStations) <- ~ long + lat
  proj4string(airStations) <- CRS("+proj=longlat +ellps=WGS84")
  ## Measurements data
  airQuality <- read.csv2('data/airQuality.csv')
  ## Only interested in NO2 
  NO2 <- airQuality[airQuality$codParam==8, ]
  ## Auxiliary function to calculate aggregate values
  summarize <- function(formula, data,
                        FUN=function(x)c(mean=mean(x), median=median(x), sd=sd(x)),
                        ...){
    agg <- aggregate(formula, data, FUN=FUN, ...)
    data.frame(do.call(cbind, agg))
  }
  
  NO2agg <- summarize(dat ~ codEst, data=NO2)
  
  ## Link aggregate data with stations to obtain a SpatialPointsDataFrame.
  ## Codigo and codEst are the stations codes
  idxNO2 <- match(airStations$Codigo, NO2agg$codEst)
  airStations <- spCbind(airStations, NO2agg[idxNO2, ])
#+end_src

*** Proportional symbol with =spplot=

The =airStations= =SpatialPointsDataFrame= can be easily displayed
with the =spplot= method provided by the =sp= package, based on
=xyplot= from the =lattice= package. Both color and size can be
combined in a unique graphical output because =spplot= accepts
both of them (figure \ref{airMadrid_spplot}). I define a
sequential palette whose colours denote the value of the variable
(green for lower values of the contaminant, brown for intermediate
values and black for highest values).

## TODO cex!!!

#+CAPTION: Annual average of $NO_2$ measurements in Madrid. Values are shown with different symbols sizes and  colors for each class with the =spplot= function.
#+LABEL: fig:airMadrid_spplot
#+begin_src R :results output graphics :exports both :file figs/airMadrid_spplot.pdf
  airPal <- colorRampPalette(c('springgreen1', 'sienna3', 'gray5'))(5)
  
  spplot(airStations["mean"], col.regions=airPal, cex=sqrt(1:5),
         edge.col='black', scales=list(draw=TRUE),
         key.space='right')
#+end_src

The =ggplot2= version of this code needs to transform the
=SpatialPointsDataFrame= to a conventional =data.frame= (which
will contain two columns with latitude and longitude values).
#+begin_src R :eval no-export
  airStationsDF <- data.frame(airStations)
  airStationsDF$Mean <- cut(airStations$mean, 5)
  
  ggplot(data=airStationsDF, aes(long, lat, size=Mean, fill=Mean)) +
      geom_point(pch=21, col='black') + theme_bw() +
      scale_fill_manual(values=airPal)
#+end_src

*** Optimal classification and sizes to improve discrimination

Two main improvements can be added to the figure
\ref{airMadrid_spplot}:

- Define classes dependent on the data structure (instead of the
  uniform distribution assumed with =cut=). A suitable approach is
  the =classInterval= function of the =classInt= package, which
  implements the Fisher-Jenks optimal classification
  algorithm. 
\index{Packages!classInt@\texttt{classInt}}
\index{classIntervals@\texttt{classIntervals}}
\index{findCols@\texttt{findCols}}
\index{findColours@\texttt{findColours}}
#+begin_src R 
  library(classInt)
  ## The number of classes is chosen between the Sturges and the
  ## Scott rules.
  nClasses <- 5
  intervals <- classIntervals(airStations$mean, n=nClasses, style='fisher')
  ## Number of classes is not always the same as the proposed number
  nClasses <- length(intervals$brks) - 1
#+end_src

#+begin_src R :exports both
op <- options(digits=4)
tab <- print(intervals)
options(op)
#+end_src

- Encode each group with a symbol size (circle area) such that
  visual discrimination among classes is enhanced. The next code
  uses the set of radii proposed by Borden Dent as detailed in
  \cite{Slocum.McMaster.ea2005}.
#+begin_src R 
  ## Complete Dent set of circle radii (mm)
  dent <- c(0.64, 1.14, 1.65, 2.79, 4.32, 6.22, 9.65, 12.95, 15.11)
  ## Subset for our dataset
  dentAQ <- dent[seq_len(nClasses)]
  ## Link Size and Class: findCols returns the class number of each
  ## point; cex is the vector of sizes for each data point
  idx <- findCols(intervals)
  cexNO2 <- dentAQ[idx]
#+end_src

#+begin_src R :exports graphic :tangle no#+begin_src R :results output graphics :file figs/dent.pdf
  xyplot(xx~1, cex=dent,
         scales=list(draw=FALSE), xlab='', ylab='',
         pch=21, fill='midnightblue', col='black',
         aspect=5/1)
#+end_src

We will now display the categorical variable =classNO2= (instead
of =mean=) whose levels are the intervals previously computed with
=classIntervals=.

#+begin_src R :exports both
  airStations$classNO2 <- factor(names(tab)[idx])
  
  airStations[,c('codEst', 'mean', 'classNO2')]
#+end_src

#+begin_src R :eval no-export
  ## spplot version
  spplot(airStations["classNO2"],
         col.regions=airPal, cex=dentAQ, 
         edge.col='black', 
         scales=list(draw=TRUE), key.space='right')
  
  ## ggplot2 version
  airStationsDF <- data.frame(airStations)
  
  ggplot(data=airStationsDF, aes(long, lat, size=classNO2, fill=classNO2)) +
      geom_point(pch=21, col='black') + theme_bw() +
      scale_fill_manual(values=airPal) +
      scale_size_manual(values=dentAQ*2)
  
#+end_src

These two enhancements are included in the figure
\ref{fig:airMadrid_classes} (which uses an improved legend).

#+CAPTION: Annual average of $NO_2$ measurements in Madrid.  
#+LABEL: fig:airMadrid_classes
#+begin_src R :results output graphics :exports both :file figs/airMadrid_classes.pdf
  ## Definition of an improved key with title and background
  NO2key <- list(x=0.98, y=0.02, corner=c(1, 0),
                title=expression(NO[2]~~(paste(mu, plain(g))/m^3)),
                cex.title=.95,
                background='gray92')
  
  pNO2 <- spplot(airStations["classNO2"],
                 col.regions=airPal,  cex=dentAQ,
                 edge.col='black',
                 scales=list(draw=TRUE),
                 key.space=NO2key)
  pNO2
#+end_src 

*** Spatial context with underlying layers and labels

The spatial distribution of the stations is better understood if
we add underlying layers with information about the spatial
context. 

A suitable method is to download data from a provider as Google
Maps or OpenStreetMap and transform it adequately. There are
several packages which provide an interface to query several map
servers. On one hand, =RGoogleMaps=, =OpenStreetMaps= and =ggmap=
provide raster images from static maps obtained from Google Maps[fn:9],
Stamen, OpenStreetMap, etc.; on the other hand, =osmar= is able to
access OpenStreetMap data and to convert it into classes provide
by existing R packages (mainly, =sp= and =igraph0= objects). 

Amongt these options I have chosen the Stamen watercolor maps
available through the =ggmap= package. It is worth to note that
these map tiles are published by Stamen Design under a Creative
Commons licence CC BY-3.0 (Attribution). They produce these maps
with data by OpenStreetMap also published under a Creative Commons
licence BY-SA (Attribution-ShareAlike).

#+begin_src R 
  library(ggmap)
  madridBox <- bbox(airStations)
  madrid <- get_map(c(madridBox), maptype='watercolor', source='stamen')
#+end_src

#+begin_src R 
  airStationsDF <- data.frame(airStations)
  
  ggmap(madrid) +
      geom_point(data=airStationsDF,
                 aes(long, lat, size=classNO2, fill=classNO2),
                 pch=21, col='black') +
         scale_fill_manual(values=airPal) +
         scale_size_manual(values=dentAQ*2)  
#+end_src
Although =ggmap= is designed to work with the =ggplot2= package,
the result of =get_map= is only a =raster= object with
attributes. Therefore, it can be easily displayed with
=grid.raster= as an underlying layer of previous the =spplot=
result.
#+begin_src R 
  ## the 'bb' attribute stores the bounding box of the get_map result
  bbMap <- attr(madrid, 'bb')
  ## This information is needed to resize the image with grid.raster
  height <- with(bbMap, ur.lat - ll.lat)
  width <- with(bbMap, ur.lon - ll.lon)
  
  pNO2 + layer(grid.raster(madrid,
                            width=width, height=height,
                            default.units='native'),
               under=TRUE)
#+end_src

A different approach is to use digital vector data (points, lines
and polygons). A popular format for vectorial data is the
shapefile, commonly used by public and private providers to
distribute the information. A shapefile can be read with
=readShapePoly= and =readShapeLines= from the =rgdal=
package. These functions produce a =SpatialPolygonsDataFrame= and
a =SpatialLinesDataFrame=, respectively, that can be displayed
with the =sp.polygons= and =sp.points= functions provided by the
=sp= package. 

For our example, the Madrid district and streets are available as
shapefiles from the nomecalles web service[fn:1].

\index{Data!nomecalles}
\index{Madrid}
\index{spTransform@\texttt{spTransform}}
\index{readShapeLines@\texttt{readShapeLines}}
\index{layer@\texttt{layer}}
\index{+.trellis@\texttt{+.trellis}}
\index{sp.polygons@\texttt{sp.polygons}}
\index{sp.pointLabel@\texttt{sp.pointLabel}}
\index{sp.lines@\texttt{sp.lines}}
#+begin_src R :eval no-export
  ## nomecalles http://www.madrid.org/nomecalles/Callejero_madrid.icm
  ## Form at http://www.madrid.org/nomecalles/DescargaBDTCorte.icm
  
  ## Madrid districts
  unzip('Distritos de Madrid.zip')
  distritosMadrid <- readShapePoly('Distritos de Madrid/200001331')
  proj4string(distritosMadrid) <- CRS("+proj=utm +zone=30")
  distritosMadrid <- spTransform(distritosMadrid, CRS=CRS("+proj=longlat +ellps=WGS84"))
  
  ## Madrid streets
  unzip('Callejero_ Ejes de viales.zip')
  streets <- readShapeLines('Callejero_ Ejes de viales/call2011.shp')
  streetsMadrid <- streets[streets$CMUN=='079',]
  proj4string(streetsMadrid) <- CRS("+proj=utm +zone=30")
  streetsMadrid <- spTransform(streetsMadrid, CRS=CRS("+proj=longlat +ellps=WGS84"))
#+end_src

#+begin_src R :exports none :tangle no
  library(rgdal)
  distritosMadrid <- readShapePoly('~/Datos/nomecalles/Distritos de Madrid/200001331')
  proj4string(distritosMadrid) <- CRS("+proj=utm +zone=30")
  distritosMadrid <- spTransform(distritosMadrid, CRS=CRS("+proj=longlat +ellps=WGS84"))
  
  ## streets <- readShapeLines('~/Datos/nomecalles/Callejero_ Ejes de viales/call2011.shp')
  ## streetsMadrid <- streets[streets$CMUN=='079',]
  ## proj4string(streetsMadrid) <- CRS("+proj=utm +zone=30")
  ## streetsMadrid <- spTransform(streetsMadrid, CRS=CRS("+proj=longlat +ellps=WGS84"))
  ## writeLinesShape(streetsMadrid, '~/Datos/nomecalles/Callejero_ Ejes de viales/streetsMadrid')
  
  streetsMadrid <- readShapeLines('~/Datos/nomecalles/Callejero_ Ejes de viales/streetsMadrid.shp')
  proj4string(streetsMadrid) <- CRS("+proj=longlat +ellps=WGS84")
#+end_src

These shapefiles together with the stations names (placed with the
=sp.pointLabel= function from the =maptools= package) can be
included in the plot with the =sp.layout= mechanism accepted by
=spplot= or with the =layer= and =+.trellis= functions from the
=latticeExtra= package. The figure \ref{fig:airMadrid} displays
the final result.
#+begin_src R :eval no-export
  spDistricts <- list('sp.polygons', distritosMadrid, fill='gray97', lwd=0.3)
  spStreets <- list('sp.lines', streetsMadrid, lwd=0.05)
  spNames <- list(sp.pointLabel, airStations, labels=airStations$Nombre,
                  cex=0.6, fontfamily='Palatino')
  
  spplot(airStations["classNO2"], col.regions=airPal, cex=dentAQ,
         edge.col='black', alpha=0.8,
         sp.layout=list(spDistricts, spStreets, spNames),
         scales=list(draw=TRUE),
         key.space=NO2key)
  
#+end_src

#+CAPTION: Annual average of $NO_2$ measurements in Madrid. Values are shown with proportional symbol mapping.
#+LABEL: fig:airMadrid
#+begin_src R :results output graphics :exports both :file figs/airMadrid.png :width 4000 :height 4000 :res 600
  pNO2 +
      layer_({
          sp.polygons(distritosMadrid, fill='gray97', lwd=0.3)
          sp.lines(streetsMadrid, lwd=0.05)
          sp.pointLabel(airStations, labels=airStations$Nombre,
                        cex=0.6, fontfamily='Palatino')
      })
#+end_src

The =ggplot2= package is not able to work directly with
=SpatialLinesDataFrame= or =SpatialPolygonDataFrame=
objects. Instead, it includes several =fortify= methods to convert
objects from these classes into a conventional
=data.frame=. Unfortunately, the =fortify= process for the
=SpatialLinesDataFrame= of our example requires too much time to
be completed. Consequently, the =ggplot2= version uses only the
district information.
#+begin_src R 
  airStationsDF <- data.frame(airStations)
  distritosMadridDF <- fortify(distritosMadrid)
  
  ggplot(data=airStationsDF, aes(long, lat, size=classNO2, fill=classNO2)) +
      geom_point(pch=21, col='black') + theme_bw() +
      scale_fill_manual(values=airPal) +
      scale_size_manual(values=dentAQ*2) +
      geom_line(distritosMadridDF)
      
  
#+end_src

*** \floweroneleft Additional information with tooltips and hyperlinks

Now, let's suppose you need to know the median and standard deviation
of the time series of that large brown station on the left. Moreover,
you would like to watch a photography of that station, or even better,
you wish to visit its webpage for additional information. A frequent
solution is to produce interactive graphics with tooltips and
hyperlinks.

The =gridSVG= package is able to create an SVG graphic, where each
component owns a =title= attribute: the content of this attribute
is commonly displayed as a tooltip when the mouse hovers over the
element. The content of this attribute can be modified thanks to
the =grid.garnish= function. Moreover, the =grid.hyperlink=
function can add the hyperlinks to the webpage of each station the
correspondent graphical element. 

The tooltips will display the photography of the station, the name
of the station, and the statistics previously calculated with
=aggregate= in the first step of this section.  The station images
are downloaded from the Munimadrid webpage. The =htmlParse=
function from the =XML= package parses each station page, and the
station photograph is extracted with =getNodeSet= and =xmlAttrs=.

\index{Packages!XML@\texttt{XML}}
\index{htmlParse@\texttt{htmlParse}}
\index{getNodeSet@\texttt{getNodeSet}}
#+begin_src R :eval no-export
  library(XML)

  old <- setwd('images')
  for (i in 1:nrow(airStationsDF)){
    codEst <- airStationsDF[i, "codEst"]
    ## Webpage of each station
    codURL <- as.numeric(substr(codEst, 7, 8))
    rootURL <- 'http://www.mambiente.munimadrid.es'
    stationURL <- paste(rootURL,
                        '/opencms/opencms/calaire/contenidos/estaciones/estacion',
                        codURL, '.html', sep='')
    content <- htmlParse(stationURL, encoding='utf8')
    ## Extracted with http://www.selectorgadget.com/
    xPath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "imagen_1", " " ))]'
    imageStation <- getNodeSet(content, xPath)[[1]]
    imageURL <- xmlAttrs(imageStation)[1]
    imageURL <- paste(rootURL, imageURL, sep='')
    download.file(imageURL, destfile=paste(codEst, '.jpg', sep=''))
  }
  setwd(old)
#+end_src

Next, we attach the hyperlink and the SVG information to each
circle. 
\index{Packages!gridSVG@\texttt{gridSVG}}
\index{JavaScript}
\index{grid.garnish@\texttt{grid.garnish}}
\index{grid.hyperlink\texttt{grid.hyperlink}}
\index{gridToSVG\texttt{gridToSVG}}
#+begin_src R 
  print(pNO2 + layer_(sp.polygons(distritosMadrid, fill='gray97', lwd=0.3)))
#+end_src

#+begin_src R 
  library(gridSVG)
  
  airStationsDF <- as.data.frame(airStations)
  
  tooltips <- sapply(seq_len(nrow(airStationsDF)), function(i){
    codEst <- airStationsDF[i, "codEst"]
    ## Information to be attached to each line
    stats <- paste(c('Mean', 'Median', 'SD'),
                   signif(airStationsDF[i, c('mean', 'median', 'sd')], 4),
                   sep=' = ', collapse='<br />')
    ## Station photograph 
    imageURL <- paste('images/', codEst, '.jpg', sep='')
    imageInfo <- paste("<img src=", imageURL,
                       " width='100' height='100' />", sep='')
    ## Text to be included in the tooltip
    nameStation <- paste('<b>', 
                         as.character(airStationsDF[i, "Nombre"]),
                         '</b>', sep='')
    info <- paste(nameStation, stats, sep='<br />')
    ## Tooltip includes the image and the text
    paste(imageInfo, info, sep='<br />')
  })
  grid.garnish('points.panel', title=tooltips,  grep=TRUE, group=FALSE)
#+end_src

#+begin_src R 
  ## Webpage of each station
  rootURL <- 'http://www.mambiente.munimadrid.es'
  urlList <- sapply(seq_len(nrow(airStationsDF)), function(i){
    codEst <- airStationsDF[i, "codEst"]
    codURL <- as.numeric(substr(codEst, 7, 8))
    stationURL <- paste(rootURL,
                        '/opencms/opencms/calaire/contenidos/estaciones/estacion',
                        codURL, '.html', sep='')
    })
  
  grid.hyperlink('points.panel', urlList, grep=TRUE, group=FALSE)
#+end_src

The =title= attribute can be accessed with the JavaScript plugins
jQuery[fn:8] and qTip[fn:6] to display tooltips when the mouse
hovers over each station. The =grid.script= function creates
objects containing links to these plugins. =gridToSVG= uses these
objects to produce an SVG document with script elements.

\index{jQuery} 
\index{qTip}
#+begin_src R
  grid.script(file='http://code.jquery.com/jquery-1.8.0.min.js')
  grid.script(file='js/jquery.qtip.js')
  ## Simple JavaScript code to initialize the qTip plugin
  grid.script(file='js/myTooltip.js')
  ## Produce the SVG graphic: the results of grid.garnish,
  ## grid.hyperlink and grid.script are converted to SVG code
  gridToSVG('figs/airMadrid.svg')
#+end_src

These plugins will work only after the file =airMadrid.svg=
created by =gridToSVG= is inserted in a HTML file with standard
headers. Figure \ref{fig:airMadridTooltip} shows a capture of the
result.
#+begin_src R
  htmlBegin <- '<!DOCTYPE html>
  <html>
  <head>
  <title>Air Quality in Madrid</title>
  <link rel="stylesheet" type="text/css" href="stylesheets/jquery.qtip.css" />
  </head>
  <body>'
  
  htmlEnd <- '</body>
  </html>'
  
  svgText <- paste(readLines('figs/airMadrid.svg'), collapse='\n')
  
  writeLines(paste(htmlBegin, svgText, htmlEnd, sep='\n'),
             'airMadrid.html')
#+end_src


#+BEGIN_LaTeX
\begin{figure}
\includegraphics[width=0.9\textwidth]{figs/airMadridTooltip.png}
\caption{\label{fig:airMadridTooltip}Tooltips generated with \texttt{gridSVG} using jQuery and qTip2}
\end{figure}
#+END_LaTeX


** Vector fields

#+begin_src R 
library(raster)
library(rasterVis)
#+end_src

#+begin_src R :eval no-export
  old <- setwd(tempdir())
  download.file('ftp://tgftp.nws.noaa.gov/SL.us008001/ST.expr/DF.gr2/DC.ndfd/AR.conus/VP.001/ds.wdir.bin', 'windDir.bin')
  download.file('ftp://tgftp.nws.noaa.gov/SL.us008001/ST.expr/DF.gr2/DC.ndfd/AR.conus/VP.001/ds.wspd.bin', 'windSpeed.bin')
    
  wDir <- raster('windDir.bin')/180*pi
  wSpeed <- raster('windSpeed.bin')
  
  setwd(old)
#+end_src

#+begin_src R :exports none :tangle no
  wDir <- raster('~/Datos/windUSA/windDir.bin')/180*pi
  wSpeed <- raster('~/Datos/windUSA/windSpeed.bin')
#+end_src

#+begin_src R 
idxNA <- (wDir == 9999) | (wSpeed == 9999) | (wSpeed <= 0)

wDir[idxNA] <- NA
wSpeed[idxNA] <- NA
#+end_src


#+begin_src R 
  wind <- stack(wSpeed, wDir)
#+end_src

#+begin_src R 
  levelplot(wind, layers='windSpeed',
            par.settings=BTCTheme(),
            scales=list(draw=FALSE))
#+end_src

#+begin_src R 
  library(car)
  
  boxCox <- function(x){
    lambda <- powerTransform(x~1)
    res <- bcPower(x, coef(lambda))
    }
  
  wSpeed[] <- boxCox(wSpeed[])
#+end_src

#+begin_src R 
  windField <- stack(wSpeed, wDir)
  names(windField) <- c('slope', 'aspect')
#+end_src

#+begin_src R 
  vectorplot(windField, isField=TRUE, par.settings=BTCTheme(),
             colorkey=FALSE, scales=list(draw=FALSE))
  
#+end_src

#+begin_src R 
  streamplot(windField, isField=TRUE,
             droplet=list(pc=.2), streamlet=list(L=25),
             scales=list(draw=FALSE))
#+end_src

#+begin_src R 
  altUSA <- raster('~/Datos/USA_msk_alt/USA1_msk_alt')
#+end_src
#+begin_src R 
  levelplot(altUSA, par.settings=BTCTheme())
#+end_src

* Footnotes

[fn:1] [[http://www.madrid.org/nomecalles/Callejero_madrid.icm]]

[fn:2] The =sp= package includes the =bubble= function to produce proportional symbol maps. However, for a better control of the result we will write our own code.

[fn:3]
  [[http://www.nytimes.com/interactive/2009/03/10/us/20090310-immigration-explorer.html]]. Several
  infographics of this newspaper can be found at [[http://www.smallmeans.com/new-york-times-infographics/]].

[fn:5] [[http://www.infoelectoral.mir.es/docxl/04_201105_1.zip]]

[fn:7] [[http://www.ine.es]]

[fn:4] A more detailed analysis of this dataset can be found at
  [[http://prezi.com/fipcfjen_fck/pongamos-que-hablo-del-aire-de-madrid/]]
  (in Spanish).

[fn:6] http://craigsworks.com/projects/qtip2/

[fn:8] [[http://jquery.com/]]

[fn:9] You should read the Google Maps Terms of Service before
  using their data for your project.
